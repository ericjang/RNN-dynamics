\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

% my extra stuff
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsfonts} % mathbb support
\usepackage{amsmath} % multiline equations
\usepackage{amssymb} % \intercal (transpose)
\usepackage{hyperref} % hyperlinks

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}


\title{Recurrent Neural Networks : a Dynamical Systems Perspective}

\author{
Eric V. Jang \\
APMA136 Term Paper \\
Brown University \\
Providence, RI 02912 \\
\texttt{eric\_jang@brown.edu} \\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the scond
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\bf}[1]{\mathbf{#1}}
\newcommand{\x}{\bf{x}}
%\newcommand{\r}{\bf{r}}
\newcommand{\W}{\bf{W}}
\newcommand{\xstar}{\bf{x^\star}}
\newcommand{\F}{\bf{F}}
\newcommand{\J}{\bf{J}}
\newcommand{\p}{\partial}
\newcommand{\pf}[2]{\frac{\p{#1}}{\p{#2}}}
\newcommand{\ppf}[3]{\frac{\p^2#1}{\p#2\p#3}}
\nipsfinalcopy % Uncomment for camera-ready version



\begin{document}


\maketitle

\begin{abstract}
Recurrent Neural Networks (RNNs) are artificial neural networks with feedback connections. The operation of an RNN generates a dynamic temporal state, which functions as an internal memory. Such architectures are well-suited for learning complex input-output sequences, such as generative models for speech, motor control, and time series prediction. Since RNNs incorporate state evolution over time, they can be analyzed as nonlinear dynamical systems. Understanding the phase portrait of an RNN may provide us with insight as to its memory capcity and stability of computation. This work implements a conjugate-gradient method for numerically finding fixed points in Echo State Networks, and analyzes the role of supervised learning on fixed points.
\end{abstract}

\section{Introduction} % no equations here

Artificial Neural Networks (ANNs) are a family of neurally-inspired information processing systems that perform nonlinear transformations on high-dimensional inputs. Generally, an ANN is a collection of ``neuronal units'' that are simulated over time. Each unit integrates activations from its ``presynaptic'' units in order to update its own activation. The unit then forwards its activation to each of its ``postsynaptic'' neighbors, and the process starts over. Most ANN architectures satisfy the universal approximation theorem, and can perform arbitrary memory-bounded computation \cite{Hornik1991251}.

ANNs are particularly powerful because they can learn \textit{computations} on data, whereas most statistics-based machine learning models match distributions to data. In most ANN architectures, units are organized into ``layers'' to impose connectivity structure between an otherwise fully-connected network. These layers can be chained together to form complex hierarchical representations of inputs. In feed-forward networks, layers are connected in a directed, acyclic graph. Activations are propagated sequentially along the graph until they terminate at the leaf nodes. In a Recurrent Neural Network (RNN), the connectivity between layers can form a directed multigraph. Through forward propagation, network activity can loop back and revisit the same layers over and over again. This creates an internal ``memory'' state that the network can then utilize to incorporate new observations with old ones.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.3\textheight]{images/ff.png}
  \caption{Feedforward Neural Network}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.3\textheight]{images/rnn.png}
  \caption{Recurrent Neural Network (RNN)}
  \label{fig:sub2}
\end{subfigure}
\caption{Example of two Artificial Neural Networks (ANNs)}
\label{fig:test}
\end{figure}

Thanks to modern training techniques for neural networks, RNN models have shown promise in speech recognition, handwriting recognition, and content-addressable memory tasks \cite{DBLP:journals/corr/HannunCCCDEPSSCN14, DBLP:journals/corr/GravesWD14}. The term ``Deep Learning'' describes this trendy intersection of big data, neural networks, and high-performance computing.

Despite these improvements, it is not well understood how RNNs learn and implement their desired computations. How many feedforward layers are needed? How far back can the model remember? What exactly is being represented internally? Motivated by a general lack of understanding of \textit{how} these ``black box'' models learn and compute, I set out to analyze RNNs from a dynamical systems perspective, as done  in Sussillo \& Barak2014 \cite{SussilloBarak2014253}. As an extension to their work, I also investigate the effects of training, activation functions, and transfer learning on the arrangement of fixed points in the system.

\section{Notation}

\begin{itemize}
  \item Boldfaced symbols $\x$ denote a vector or a matrix.
  \item $\bf{A} \in \mathbb{R}^{M \times N}$ refers to a matrix $A$ with M rows and N columns.
  \item Subscripts $\F_i$ denote the $i$'th element of the vector $\F$. Multiple subscripts in $\bf{M}_{rc}$ denote the value at row $r$, column $c$ of matrix $M$.
  \item Superscripts are used to distinguish variables from each other (i.e. $\bf{W}^{FB}$ and $\bf{W}^{o}$ are both weight matrices)
\end{itemize}

\section{Finding Fixed and Slow Points}

Fixed points are often the first line of inquiry for characterizing the phase portrait of a system. Once found, analysis of linearized dynamics about these fixed points can reaveal how a local portion of the system behaves.

Let $\x \in \mathbb{R}^N$ be a N-dimensional state vector, with dynamics governed by the following set of first-order, differential equations:

\begin{equation}
\dot{\x} = \frac{d\F}{dt} = \F(\x)
\end{equation}

A fixed point $\xstar \in \mathbb{R}^N$ then satisfies the following:

\begin{equation} \label{eq:Fq}
  \F(\xstar) = \bf{0}
\end{equation}

Consider the Taylor expansion of Equation (\ref{eq:Fq}) about the point $\x = \xstar+\bf{\delta}$:

\begin{equation} \label{eq:Taylor}
  \F(\xstar+\bf{\delta}) = \F(\xstar) + \F'(\xstar)\bf{\delta} + \frac{1}{2}\F''(\xstar)\bf{\delta}^2 +...
\end{equation}

When $\bf{\delta}$ is sufficiently small, the first-order terms of $F(\xstar+\bf{\delta})$ dominate the higher-order terms in Equation (\ref{eq:Taylor}), so $\F(\x)$ can be approximated as a linear system nearby fixed points.

Let objective function $q \in \mathbb{R}$ be defined as:

\begin{equation} \label{eq:q}
  q(\x)=\frac{1}{2}{|\F(\x)|^2}=\frac{1}{2}\sum_{i=1}^{N}{\F_i(\x)^2}
\end{equation}

Observe that $q(\x) > 0$ for all $x \neq \xstar$. By finding the $\x$ that minimizes this potential system, we can find the slowest regions of the system, whether it be slow or fixed points.

In this work, I chose the conjugate gradient (CG) method to minimize $q(\x)$. It should be noted that gradient descent, which is a first-order optimization technique, typically arrives at the same minima as a second-order method (in practice). However, first-order descent occurs extremely slowly, because computation time is wasted repeatedly moving down the same direction, or oscillating back and forth across valleys in the energy surface. In contrast, the CG method picks a sequence of orthogonal search directions such that it jumps to the basin of a locally quadratic error surface after a fixed number of steps. This leads to faster convergence times.

One drawback of the CG method is that it requires the constant evaluation of the gradient $\nabla q(\x)$ and Hessian $\bf{H}$ with respect to $\x$. For large systems this can become cumbersome; however, the examples dealt with in this work are tractable enough to evaluate the Jacobian and Hessian matrices in full.

To compute the gradient, we use Equation (\ref{eq:q}) and the chain rule:

\begin{align}
  \nabla q(\x)_i &= \pf{q}{\x_i} \\
  &= \frac{2}{2}{\F_1(\x)\pf{\F_1(x)}{\x_i}} + \frac{2}{2}{\F_2(\x)\pf{\F_2(x)}{\x_i}} + ... +
  \frac{2}{2}{\F_N(\x)\pf{\F_N(x)}{\x_i}} \\
  &=\dot{\x_1}\pf{\F_1(x)}{\x_i} +
  \dot{\x_2}\pf{\F_2(x)}{\x_i}+...\dot{\x_N}\pf{\F_N(x)}{\x_i} \\
  &=\pf{\F(x)}{\x_i}^{\intercal}\dot{x}
\end{align}

In vector notation, this can be re-written as

\begin{equation} \label{eq:Jacobian}
  \nabla q(\x) = \J^{\intercal}\dot{x}
\end{equation}

The $\dot{\x}_k$ terms in the Hessian matrix vanish as the system approaches a fixed point, so it is sufficient to use the Gauss-Newton approximation to the Hessian matrix by ignoring the second term in \ref{eq:RealHessian}.

\begin{align}
\bf{H} &= \begin{pmatrix}
\ppf{q}{\x_1}{\x_1} & \ppf{q}{\x_1}{\x_2} & \cdots & \ppf{q}{\x_1}{\x_N} \\
\ppf{q}{\x_2}{\x_1} & \ddots &  & \vdots \\
 &  &  & \\
\ppf{q}{\x_N}{\x_1} & \cdots &  & \ppf{q}{\x_N}{\x_N}
\end{pmatrix} \\
\ppf{q}{\x_i}{\x_j} &= \sum_{k=1}^{N}\pf{\F_k}{\x_i}\pf{\F_k}{\x_j} + \sum_{k=1}^{N}{\dot{\x}_k\pf{\F_k}{\x_i}\pf{\F_k}{\x_j}} \label{eq:RealHessian} \\
&\approx \sum_{k=1}^{N}\pf{\F_k}{\x_i}\pf{\F_k}{\x_j}
\end{align}

In vector notation, this is

\begin{equation} \label{eq:Hessian}
  \bf{H} = \J^{\intercal}\J
\end{equation}


%%%% SIMPLE 2D EXAMPLE %%%%%

\subsection{A Simple 2D Example}

The following two-dimensional toy system illustrates the use of the $q(\x)$ minimization technique for finding fixed points. Our system is given by:

\begin{align} % asterix determines whether it is numbered or not
\dot{x} = (1-x^2)y  \label{eq:simple2d:x}  \\
\dot{y} = \frac{x}{2}-y  \label{eq:simple2d:y}
\end{align}

Solving for $x$-nullclines, we have $\dot{x} = 0 \iff y = 0 \lor x = \pm 1$. Solving for the $y$-nullclines, we have $\dot{y} = 0 \iff y = \frac{1}{2}x$. The fixed points are located at the intersection of the $x$ and $y$ nullclines: $(1,\frac{1}{2})$, $(1,-\frac{1}{2})$, and $(0,0)$.

The Jacobian is given by:

\begin{align}
  \bf{J}(x,y) &= \begin{pmatrix}
    \pf{f}{x} & \pf{f}{y}\\
    \pf{g}{x} & \pf{g}{y}
    \end{pmatrix}
    \\
&= \begin{pmatrix}
    -2xy & 1-x^2\\
    \frac{1}{2} & -1
    \end{pmatrix}
\end{align}

Evaluating $J$ at $(1,\frac{1}{2})$, $(1,-\frac{1}{2})$, and $(0,0)$ and computing eigenvalues reveal a stable node, a stable node, and a saddle, respectively.

Figure \ref{fig:simple2d} shows that results of $q(\x)$ minimization are in agreement with the analytical results.

\begin{figure}
\centering
\includegraphics[height=.5\textwidth]{images/simple2d.png}
\caption{Contour plot of $q$ as a function of $x$ and $y$, with phase portrait of the system in Equations (\ref{eq:simple2d:x}, \ref{eq:simple2d:y}). Fixed points found via $q(\x)$ minimization are plotted in red. A saddle node at $(0,0)$ funnels incoming trajectories towards either one of the stable, attracting nodes.}
\label{fig:simple2d}
\end{figure}

%%%% BEGIN RNNs %%%%%

\section{Jacobian and Hessian of a simple RNN}

Consider a simplified RNN, as depicted in Figure (\ref{fig:simple_rnn}).  As in the simple 2D example, we would like to derive analytical expressions for the RNN's kinetic energy $q(\x)$, as well as the gradient $\nabla q(\x)$ and (approximate) Hessian $\bf{H}$.

\begin{figure}
\centering
\includegraphics[height=.3\textwidth]{images/rnn_simple.png}
\caption{Simple RNN}
\label{fig:simple_rnn}
\end{figure}

The dynamics of this particular RNN architecture are as follows:

\begin{equation} \label{eq:simpleRNN}
  \bf{F}_i(\x) &=-\x_i + \sum_k^N{\bf{W^r}}_{ik}\bf{r}_k
\end{equation}
\begin{equation}
  \bf{r}(\x) = tanh(\x)
\end{equation}

The $ith$ row of the Jacobian matrix is then given by

\begin{equation}
  \pf{\bf{F}_i(\x)}{\x_j} &= -\delta_{ij} + \bf{W^r}_{ij}\pf{\bf{r}(\x_j)}{\x_j}
\end{equation}

Where $\delta_{ij} \iff i = j$. For clarity, the form of the Jacobian is shown in Equation (\ref{eq:JacRNN}).

\begin{equation} \label{eq:JacRNN}
  \bf{J} = \begin{pmatrix}
    \square & \square & \square \\
    \square & \square & \square \\
    \square & \pf{\F_i}{\x_j} & \square
  \end{pmatrix} = \begin{pmatrix}
  \square & \square & \square \\
  \square & \square & \square \\
  \square & W^r_{ij} & \square
\end{pmatrix} .* \begin{pmatrix}
\square & \square & \square \\
\square & \square & \square \\
\pf{\bf{r}}{\x_i} & \pf{\bf{r}}{\x_j} & \pf{\bf{r}}{\x_k}
\end{pmatrix} - \bf{I}
\end{equation}

The nonlinear activation function $r(\x_j)$ and its derivative $\pf{\bf{r}}{\x_k}$ with respect to $\x_j$ are $tanh(\x_j)$ and $1-tanh(\x_j)^2$, respectively.

Once we obtain $\J$, it is straightforward to compute $\nabla q(\x)$ and $\bf{H}$ via Equations (\ref{eq:Jacobian}) and (\ref{eq:Hessian}).

\section{Echo State Networks (ESNs)}

We now consider a more complex RNN architecture: the Echo State Network (ESN). As shown in Figure (\ref{fig:esn_arch}), an ESN receives inputs $\bf{u} \in \mathbb{R}^I$, emits outputs $\bf{z} \in \mathbb{R}^O$, maintains activations $\x \in \mathbb{R}^N$ and firing rates $\bf{r}(\x) \in \mathbb{R}^N$. $\bf{r}(\x)$ is the element-wise application of a sigmoid nonlinearity (such as the hyperbolic tangent) onto $\x$. A network diagram is depicted in Figure \ref{fig:esn_arch}.

\begin{figure}
\centering
\includegraphics[height=.7\textwidth]{images/esn.png}
\caption{An Echo State Network (ESN) is an RNN that is trained by only varying the output weights, and feeding the output signal back into the network.}
\label{fig:esn_arch}
\end{figure}

$W^r \in \mathbb{R}^{N \times N}$ is the matrix of recurrent weights projecting $\bf{r}$ back onto $\x$. $\bf{W}^{FB} \in \mathbb{R}^{N \times O}$ are the feedback weights projecting the output back into the activations.
$\bf{B} \in \mathbb{R}^{N x I}$ are the weights for the input layer to the activation layer. Finally, $W^o \in \mathbb{R}^{O \times N}$ are the readout weights from the rates to the output $\bf{z}$.

% describe eqn for echo-state
The activation state vector $\x$ obeys the following dynamics:

\begin{align}
  \bf{F}(\x) &= -\x + \bf{W}_{r}\bf{r}+\bf{W}_{fb}{\bf{z}}+\bf{B}\bf{u} \label{eq:ESN} \\
  \bf{z} &= \bf{W}_o \bf{r} \label{eq:z}
\end{align}

Our next task is to compute the Jacobian of $q(\x)$ for this particular system. Notice that we can take advantage of our previous Jacobian derviation in Equation (\ref{eq:JacRNN}) by substituting Equation (\ref{eq:z}) into Equation (\ref{eq:ESN}) and factoring out the $\bf{r}$ term:

\begin{align}
  \bf{F}(\x) &= -\x + \bf{W}_{r}\bf{r}+\bf{W}_{fb}{\bf{W}_o \bf{r}}+\bf{B}\bf{u} \\
  \bf{F}(\x) &= -\x + (\bf{W}_{r}+\bf{W}_{fb}\bf{W}_o)\bf{r}+\bf{B}\bf{u} \\
  &= -\x + \bf{W}^c\bf{r}+\bf{B}\bf{u}
\end{align}

The form of the Jacobian is computed in exactly the same way as that of the simpler RNN model, except $\bf{W}^r$ is replaced with the combined matrix $W^c = (\bf{W}_{r}+\bf{W}_{fb}\bf{W}_o)$. The input term $\bf{B}\bf{u}$ does not depend on $\x$, and so it drops out from the Jacobian.

\section{Three-Bit Flip-Flop Task}

The ESN network consists of an activation state with $N=1000$ units, three input channels and three corresponding output channels. The three input channels independently hold values of $0$, but pulse briefly at random intervals with a value of $+1$ or $-1$. The ESN is trained to hold each of its outputs equal to the value of the last input pulse on the corresponding channel. Refer to Figure \ref{fig:train_result}.


\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.3\textheight]{images/train3_tanh_zft.png}
  \caption{Output of network (red) exactly matches training target (green)}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.3\textheight]{images/train3_tanh_test.png}
  \caption{Testing Target and Ouput for 3-bit Flip-Flop}
\end{subfigure}
% empty space
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.3\textheight]{images/train3_tanh_mse.png}
  \caption{MSE during training is kept small by rapid FORCE learning updates.}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.3\textheight]{images/train3_tanh_wo.png}
  \caption{Mean weights of $\bf{W}^O$ during training.}
\end{subfigure}
\caption{}
\label{fig:train_result}
\end{figure}


FORCE-learning via Recursive Least Squares was used to train the network to fit synthetically generated training and validation sets. State vectors $\x$ were sampled randomly from test trajectories and used as initial conditions (ICs) for $q(\x)$ minimization. In agreement with existing literature, $q(\x)$ minimization revealed the existence of 9-26 fixed points, depending on the tolerance levels used.

The 1000-dimensional fixed points and test trajectories were then projected to their largest three principal components. Figure (\ref{fig:3bit_phase}) shows that the dynamics of the ESN occupy a 3D lattice, with eight attracting fixed points representing the eight distinct memory states (0,0,0), (0,0,1), (0,1,0), ... (1,1,1). Each fixed point corresponds to a memory state that is equidistant from all states one bit-flip away, so the network essentially has spaced out its fixed points in such a way that the ``energy'' from an impulse is just strong enough to push the state to the attracting set of a different fixed point.

In addition to starting from ``naturally-occuring'' test states, it is also worthwhile to characterize the dynamics of the system in regions of state space that were not visited during testing. However, what happens to an initial condition that starts inside or outside the cube?

Random output values of $\bar{\bf{z}} \in \mathbb{R}^o$ were chosen out of the bounds of stable memory states (for example, $(0.5,0.5,0.5)$ or $2.0,2.0,0.0$) and used as ICs in Q-optimization. Such trajectories are shown in Figure (\ref{fig:3bit_phase}), highlighting the structure of the central fixed point as a saddle.
)

\begin{figure}
\centering
\includegraphics[height=.5\textwidth]{images/3bit_tanh_phase.png}
\caption{Fixed points, test trajectories, and random sampled ICs projected onto the top three priincipal components. The self-organization of an attracting submanifold illustrates the memory capacity of the network (8 stable states) and the phase portrait shows possible transition paths between these states. See Supplementary Materials online to see an animated roundtable.}
\label{fig:3bit_phase}
\end{figure}

\subsection{Evolution of Phase Portrait}

How does the memory capcity of the ESN gradually emerge over the course of training? The stability and location of fixed points are determined entirely by a rank-three modification (via $\bf{W}^O$) to the combined weight matrix, so it is evident that some form of bifurcation is occuring as $\bf{W}^O$ is continuously modified via FORCE training.

In this set of experiments, a fixed set of initial states $\{\x^0_i\}$ were chosen, and for different snapshots of $\bf{W}^O$ saved during training, $q(\x)$ minimization was performed on $\{\x^0_i\}$. As shown in Figure (\ref{fig:weightbend}), the fixed point at the origin is initially stable, but the phase space is stretched like saltwater taffy into a plane attractor, homoclinic orbits, then a cube attractor.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.3\textheight]{images/evolve3/01.png}
  \caption{}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.3\textheight]{images/evolve3/02.png}
  \caption{}
\end{subfigure}
% empty space
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.3\textheight]{images/evolve3/07.png}
  \caption{}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.3\textheight]{images/evolve3/10.png}
  \caption{}
\end{subfigure}
\caption{Evolution of phase space over training iterations, with final fixed points overlaid. a) The network is initially at a resting state with a single fixed point at the origin. b) At 200 iterations, the origin is no longer an attracting node. c) At 700 iterations, the principal eigenvalues of the origin vanishes, forming homoclinic orbits (in red). The trajectory (blue) remains fixed on a single memory state. d) The formation of an unstable saddle at the center pushes  phase space (red) outwards, gradually bending the test trajectory into a cuboid shape. Full time-lapse animations of the phase space organization can be found online in the \href{https://github.com/ericjang/RNN-dynamics}{Supplementary Materials}}.
\label{fig:weightbend}
\end{figure}

\section{Four-Bit Flip-Flop}

We now train the ESN to perform the exact same task, except this time with 4 flip-flop bits rather than three. The expectation is that the fixed points settle on a tesseract in a 4-D manifold, with equidistant, single-bit transitions between neighboring states.

The projection of the learned fixed points onto the first 3 principal components is a hexagonal prism (Figure \ref{fig:net4hex}).

\subsection{Effect of Transfer Learning on Phase Structure}

ANN models often require huge amounts of data to learn patterns, and as such, there is considerable effort in the field directed towards 1) obtaining data, and 2) designing optimal learning policies.

Transfer-learning is a popular technique used to boost performance and reduce the amount of labeled training data required. In transfer learning, the network is initialized with a copy of weights learned on a related task. For instance, suppose that there are not enough images of Black Rhinos to build an accurate ``Black Rhino'' vs. ``not-Black-Rhino'' binary classifier from scratch.

Instead, one might train a convolutional neural net to recognize White Rhinos, and then remove the top layer of weights and fine-tune the network against the limited Black Rhino Dataset. Transfer learning saves redundant computation otherwise spent learning ``easy'' features, by bootstrapping top-level hierarchical representations on top of representations that occur in other similar datasets.

However, it is not clear whether transfer learning merely speeds up convergence to a minimum error, or whether there is a hidden cost to repurposing weights from a different task. The weights themselves are a dynamical system whose trajectories are decided by the supervised learning process.

When a randomly initialized ESN is trained to perform a 4-bit Flip-Flop task, the projection of fixed points lies on a hexagonal prism, such that distances between the 16 stable states are roughly equidistant. However, initializing a 4-bit ESN Flip-Flop with the weights of a 3-bit ESN Flip-Flop (extra dimensions are set to 0). After re-training against the 4-bit Flip-Flop task, the original cuboid manifold is still intact and accomodates 8 new fixed points without morphing into a hexagonal prism (Figure (\ref{fig:net4bs})).

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.7\textwidth]{images/net4.png}
  \caption{}
  \label{fig:net4hex}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=.7\textwidth]{images/net4_bs.png}
  \caption{}
\end{subfigure}
\caption{(a) Random Initialization, followed by training : tesseract projected edge-first into a hexagonal prism. (b) Transfer Learning from a 3-bit flip-flop : tesseract projected face-first into a cuboid.}
\label{fig:net4bs}
\end{figure}

\section{Robustness to Noise}

To assess the stability of memory states, trained 4-bit flip-flop ESNs were subjected to perturbations by adding Gaussian noise distributed along $(\mu=0,\sigma=\eta)$ to the recurrent weights $\bf{W}^r$, for with varying standard deviation levels $\eta$.

Networks using either the hyperbolic tangent activation function, or logistic activation function ( $r(x) = \frac{1}{1+e^-x}$) performed similarly in response to noise. However, the four-bit flip-flop bootstrapped on top of a three-bit flip-flop was more sensitive to noise, as shown in Figure \ref{fig:stabilty}. This may result from memory states in the bootstrapped network not being well-separated enough, so noise becomes more liable to send the ESN into the wrong fixed point.

\begin{figure}
\centering
\includegraphics[height=.7\textwidth]{images/stability.png}
\caption{MSE of networks trained to perform the 4-bit flip-flop task, then subjected to random Gaussian noise added to their recurrent weight matrices. Networks with sigmoid nonlinearities are the most robust to perturbations, while a network retrained from a 3-bit flip-flop performed performed the worst.}
\label{fig:stabilty}
\end{figure}

\section{Discussion}

Recurrent Neural Networks (RNNs) are suited to problems involving time-dependency of inputs, but historically, they have been hard to train and even harder to understand. This work explores the learning and computation of RNNs from a dynamical systems perspective, in the hopes that understanding the phase space can lead us to better designs for RNN architectures and training algorithms.

An Echo State Network was trained to perform a three-bit flip-flop task. Trained weights were used to derive the kinetic energy function $q(\x)$, along with its Jacobian, gradient, and Hessian. This was used in $q(\x)$ minimization to identify fixed points, which occupy a cube structure with saddles shaping the transitions from one state to another.

Next, the evolution of the phase portrait was examined as a function of training iterations. Analysis of fixed points at various stages of training progress reveal that the network learns its memories via a series of bifurcations. The single stable node at the origin undergoes a transcritical bifurcation, turning into a saddle. Saddles form along the edges of the cube to guide one memory transition to the next.

Expanding this to a four-bit flip-flop, we see that training produces a tesseract in 4D, which projects to a hexagonal prism in 3D. However, the 3D projection shape is altered and noise sensitivity heightened when the network is pre-trained from the weights of a trained 3-bit ESN. This suggests that initial weight conditions play a huge role in the ``geometry of memory''.

\subsection{Limitations}

Given an arbitrary dynamical system, the $q(x)$ minimization procedure cannot automatically distinguish between fixed points and a slow point. There is no clear-cut value for the numerical precision of a ``true fixed point''; Sussillo and Barak choose $1e^{-30}$ as the CG tolerance for ``fixed points'', and $1e^{-27}$ as the tolerance for ``slow points'', but these values are dependent on the system at in question.

Furthermore, slow and really-slow points are a nice start, but they do not offer the complete picture of the memory capacity of a RNN. RNN ``memories'' can be encoded by any attracting submanifold in the phase space, not just fixed points. For example, biological neural networks are responsible for generating fixed action patterns, such as egg-rolling behavior in some species of geese. These neural trajectories are attracting, but not invariant (as the goose eventually stops rolling the egg).

\subsection{Future Directions}

I would like to apply these dynamical systems techniques to futher explore memory capacity and organization in recurrent systems such as Neural Turing Machines (an RNN with a memory matrix) or stigmergic communication in ants (an ant lays down a pheremone and a little while later, another ant picks it up). 

\section{Methods}

This section discusses in further detail the parameters used for network simulation. Source code was written by Eric Jang under the MIT license, and can be downloaded from \url{https://github.com/ericjang/RNN-dynamics}

\subsection{Network Parameters}

Refer to Table \ref{sample-table} for parameters used.

\begin{table}[t]
\caption{Network Parameters}
\label{sample-table}
\begin{center}
\begin{tabular}{lll}
\multicolumn{1}{c}{\bf Parameter} & \multicolumn{1}{c}{\bf Value}  &\multicolumn{1}{c}{\bf Description}
\\ \hline \\
N & 1000 & Number of units in activation layer \x \\
g & 1.5 & Gain of network \\
alpha & 1 & Learning rate of the Network \\
tau & 10 & Time constant of \dot{x}\\
\delta_{\text{train}} & 2 & Interval between weight updates\\
Training Duration         &30000 & Length of each training trajectory \\
Testing Duration             &5000 & Length of each testing trajectory\\
\end{tabular}
\end{center}
\end{table}

\subsection{FORCE-Learning with Recursive Least Squares}

In an echo-state architecture, only the readout weights $\bf{W}^O$ are modified during training. The key insight of the ESN architecture is that as long as the internal state of the network can be decoded into the correct output, it is irrelevant what the internal state is doing. The recurrent weights $\bf{W}^r$ only serve the purpose of generating a resevoir of turbulent activity in the network, that can be decoded downstream into a meaningful output.

A problem inherent to training RNNs is that adjusted weights that might reduce errors in the short term might cause later errors to become huge.
In typical echo-state network training, this can be compensated for by pretending the network got the output right all the time, and feeding in the target function into the network regardless of the difference between the output and target. The intuition behind this is that if the network output is wrong, one surely does not want to feed wrong output back into the network.

FORCE (first-order reduced and controlled error) is a weight-update scheme for RNNs where output weights are modified on a time scale comprable to network simulation \cite{Sussillo2009544}. In FORCE-learning, the feedback is not clamped to the target function, but rather the weights are updated so frequently that all error mangitudes remain small.

At intervals of $\delta_{\text{train}}$, matrices $\bf{W}^O$ and $P(t)$ are updated via the following:

\begin{equation} \label{eq:FORCE1}
\bf{W^o}(t) = W^o{(t+\Delta t)} - \bf{e}_{-}(t)(\bf{P}(t)\bf{r}(t))^\intercal
\end{equation}

\begin{equation}
\bf{P}(t) = \bf{P}(t-\Delta t) - \frac{P(t-\Delta t)\bf{r}(t)\bf{r}(t)^{\intercal}P(t-\Delta t)}{1+\bf{r}^{\intercal}(t)\bf{P}(t-\Delta t)\bf{r}(t)}
\end{equation}

$\bf{e}_{-}(t)$ is output error of the current timestep prior to weight modifcation, and $P(t)$ is a running estimate of the inverse correlation matrix between the units in $\x$, which is akin to a correlation-based learning rule.

\subsection{Implementation Details}

Simulations and analysis were performed on a 4.0 Intel Core i7 workstation. MATLAB's \textit{fminunc} function was used to perfrom conjugate-gradient minimization (with trust-region). $q(\x)$ minimization took approximately 3 minutes to find 20 fixed points or ghosts.

\section*{Acknowledgments}

I thank Dr. John Gemmer for his guidance on my project proposal, and for his communicating the wonder of nonlinear dynamical systems to the APMA1360 class this semester.

\bibliographystyle{unsrt}
\bibliography{mybib}

\end{document}
